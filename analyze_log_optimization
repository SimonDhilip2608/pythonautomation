# Optimized analyze_logs method for VegasGPTService class

def analyze_logs(self, logs):
    """Analyze logs with optimized VegasGPT integration and reliable fallback."""
    if not logs:
        logger.warning("No logs provided for analysis")
        return {
            "errors": [],
            "summary": "No logs available to analyze."
        }
    
    # First, extract errors manually as a fallback system
    manual_analysis = self.extract_errors_manually(logs)
    manual_errors = manual_analysis.get("errors", [])
    logger.info(f"Manual extraction found {len(manual_errors)} errors")
    
    # If Vegas GPT is not configured, return the manual analysis
    if not self.is_configured():
        logger.warning("AI Service is not configured, using manual extraction only")
        return manual_analysis
    
    # Try to use Vegas GPT for enhanced analysis
    try:
        # Pre-process logs to highlight errors for better AI recognition
        processed_logs = []
        for log in logs:
            level = log.get('level', '').upper()
            message = log.get('message', '')
            timestamp = log.get('timestamp', '')
            service = log.get('service', '')
            
            # Mark error logs with a prefix for better AI attention
            if level in ['ERROR', 'SEVERE'] or 'Exception' in message:
                log_entry = f"[ERROR][{timestamp}][{service}] {message}"
            else:
                log_entry = f"[{level}][{timestamp}][{service}] {message}"
            
            processed_logs.append(log_entry)
        
        # Join logs with line breaks and limit total size if needed
        total_logs = len(processed_logs)
        MAX_LOGS = 300  # Adjust based on token limits and response quality
        
        if total_logs > MAX_LOGS:
            # If too many logs, prioritize error logs and add sampling
            error_logs = [log for log in processed_logs if log.startswith("[ERROR]")]
            other_logs = [log for log in processed_logs if not log.startswith("[ERROR]")]
            
            # Keep all errors if possible, then add context with other logs
            if len(error_logs) <= MAX_LOGS:
                # Can keep all errors, add some context logs
                remaining_slots = MAX_LOGS - len(error_logs)
                sample_rate = max(1, len(other_logs) // remaining_slots)
                context_logs = other_logs[::sample_rate][:remaining_slots]
                selected_logs = error_logs + context_logs
            else:
                # Too many errors, must sample them
                selected_logs = error_logs[:MAX_LOGS]
            
            log_text = "\n".join(selected_logs)
            logger.info(f"Log volume reduced from {total_logs} to {len(selected_logs)} for AI analysis")
        else:
            log_text = "\n".join(processed_logs)
        
        # Use the enhanced prompt from our previous artifact
        prompt = f"""
        You are a specialized system log analyzer for enterprise applications. Your task is to analyze the following logs from a Verizon system and identify all errors with high precision.

        SYSTEM LOGS:
        {log_text}

        INSTRUCTIONS:
        1. Identify EVERY error message in these logs
        2. For each error found:
           - Extract the EXACT error message text
           - Determine the most likely root cause based on the stack trace and context
           - Classify the severity (Critical, High, Medium, Low) based on error type and potential impact
           - Include the timestamp when the error occurred

        RESPONSE FORMAT:
        Your response must be ONLY a valid JSON object with the following structure:
        {{
            "errors": [
                {{
                    "message": "The full error message including any stack trace information",
                    "root_cause": "Technical analysis of what caused this error",
                    "severity": "Severity classification",
                    "timestamp": "When error occurred",
                    "service": "The service or component that generated the error"
                }}
            ],
            "summary": "A brief technical summary of the error patterns detected"
        }}

        IMPORTANT GUIDELINES:
        - Java exceptions like NullPointerException, SQLException, IOException are definite errors
        - Log entries with ERROR or SEVERE level must be captured
        - Include EVERY individual error - don't summarize or group similar errors
        - Focus on technical accuracy in your root cause analysis
        - If you find no errors, return an empty errors array but still provide a summary

        KNOWN ERROR PATTERNS TO WATCH FOR:
        - KafkaMessageListener errors often indicate message processing failures
        - GenericDataBaseServiceImpl errors typically relate to database connectivity or query issues
        - Exceptions in log messages indicate runtime errors
        - Error patterns containing "timeout" suggest service communication issues

        Analyze thoroughly and return ONLY the JSON response.
        """
        
        # Optimize model settings for better analysis
        data = {
            "variables": {
                "prompt": prompt
            },
            "model": "VEGAS",
            "model_settings": {
                "temperature": 0.2,  # Lower temperature for more deterministic output
                "max_output_tokens": 4000,
                "top_p": 0.95,  # Slightly more creative but still focused
                "presence_penalty": 0.1  # Slight penalty for repetition
            }
        }
        
        inference_url = f"{self.agents_url}/inference/generate"
        headers = {
            'X-api-key': self.agents_token,
            'Content-Type': 'application/json'
        }
        
        logger.debug("Sending optimized prompt to VegasGPT for analysis")
        
        # Call the API with increased timeout
        response = requests.post(inference_url, json=data, headers=headers, timeout=90)
        
        if response.status_code != 200:
            logger.error(f"AI API error: {response.status_code}")
            return manual_analysis  # Fallback to manual extraction
        
        # Parse the AI response with improved extraction
        ai_response = response.json().get('ai_response', '')
        
        try:
            # More robust JSON extraction
            json_str = self._extract_json_from_text(ai_response)
            if json_str:
                ai_analysis = json.loads(json_str)
                
                ai_errors = ai_analysis.get('errors', [])
                
                # If AI found errors, use them
                if ai_errors:
                    logger.info(f"VegasGPT found {len(ai_errors)} errors")
                    
                    # Mark these as coming from AI and add source
                    for error in ai_errors:
                        error['source'] = 'vegasgpt'
                    
                    # Compare with manual extraction - if AI missed some obvious errors, merge them
                    if len(manual_errors) > len(ai_errors) + 5:  # AI missed several errors
                        logger.warning(f"VegasGPT may have missed errors: AI found {len(ai_errors)}, manual found {len(manual_errors)}")
                        
                        # Find manual errors not detected by AI
                        ai_messages = [error.get('message', '')[:100] for error in ai_errors]
                        missed_errors = []
                        
                        for manual_error in manual_errors:
                            manual_msg = manual_error.get('message', '')[:100]
                            if not any(self._similar_messages(manual_msg, ai_msg) for ai_msg in ai_messages):
                                manual_error['source'] = 'manual-supplemental'
                                missed_errors.append(manual_error)
                        
                        # If we found missed errors, add them
                        if missed_errors:
                            logger.info(f"Adding {len(missed_errors)} manually detected errors that VegasGPT missed")
                            ai_errors.extend(missed_errors)
                            
                            # Update summary
                            ai_analysis['summary'] = f"Analysis found {len(ai_errors)} errors (combined from AI and manual extraction)"
                    
                    return {
                        "errors": ai_errors,
                        "summary": ai_analysis.get('summary', f"Found {len(ai_errors)} errors."),
                        "extraction_method": "vegasgpt_enhanced"
                    }
                else:
                    logger.warning("VegasGPT found no errors despite manual extraction finding some")
                    return manual_analysis
            else:
                logger.warning("Could not extract JSON from AI response")
                return manual_analysis
                
        except Exception as e:
            logger.error(f"Error parsing AI response: {str(e)}")
            return manual_analysis  # Fallback to manual extraction
            
    except Exception as e:
        logger.error(f"Error using AI service: {str(e)}")
        return manual_analysis  # Fallback to manual extraction

# Add these helper methods to the VegasGPTService class

def _extract_json_from_text(self, text):
    """Extract JSON from text more robustly."""
    # Try standard JSON extraction first
    json_start = text.find('{')
    json_end = text.rfind('}') + 1
    
    if json_start >= 0 and json_end > json_start:
        return text[json_start:json_end]
    
    # If that fails, try more aggressive techniques
    import re
    
    # Look for JSON object pattern
    json_pattern = r'(\{[\s\S]*\})'
    match = re.search(json_pattern, text)
    if match:
        potential_json = match.group(1)
        # Validate it's proper JSON
        try:
            json.loads(potential_json)
            return potential_json
        except:
            pass
    
    # No valid JSON found
    return None

def _similar_messages(self, msg1, msg2):
    """Check if two error messages are similar enough to be considered the same error."""
    # Simple similarity check - can be enhanced with more sophisticated methods
    if not msg1 or not msg2:
        return False
        
    # If one is a substring of the other
    if msg1 in msg2 or msg2 in msg1:
        return True
        
    # Check for common key components
    import re
    # Extract words and compare
    words1 = set(re.findall(r'\w+', msg1.lower()))
    words2 = set(re.findall(r'\w+', msg2.lower()))
    
    common_words = words1.intersection(words2)
    
    # If they share a significant portion of words
    similarity = len(common_words) / max(len(words1), len(words2))
    return similarity > 0.7  # 70% similarity threshold
